# On TPU VM (not alpha!!!!)
"""
export PYTHONPATH="$PYTHONPATH:/usr/share/models"
export USERNAME=$(whoami)
export TPU_NAME=tpu-test-${USERNAME//_}

cd ~
rm -rf ./vit-tf || true
git clone https://github.com/yf225/vit-tf.git
cd ./vit-tf

# Max fusion
TF_XLA_FLAGS="--tf_xla_enable_xla_devices --tf_xla_auto_jit=2" python3 vit_tf_tpu.py --bits=16 --micro_batch_size=4

# No fusion
TF_XLA_FLAGS="--tf_xla_enable_xla_devices --tf_xla_auto_jit=-1 --tf_xla_max_cluster_size=1" python3 vit_tf_tpu.py --bits=16 --micro_batch_size=4
"""

# !pip install -U tensorflow-addons

# -*- coding: utf-8 -*-
"""Vision Transformer with TF2.0 on TPU (using ViT from Keras tutorial)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18kyb9Z62QCA8oXnOPm6WsxC4oW48XZrS

#### License

Copyright 2019-2020 Google LLC

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


---


This is not an official Google product but sample code provided for an educational purpose.

# TPUs in Colab&nbsp; <a href="https://cloud.google.com/tpu/"><img valign="middle" src="https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png" width="50"></a>

## Enabling and testing the TPU

First, you'll need to enable TPUs for the notebook:

- Navigate to Editâ†’Notebook Settings
- select TPU from the Hardware Accelerator drop-down

Next, we'll check that we can connect to the TPU:
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
# print("Tensorflow version " + tf.__version__)

# mixed_precision_policy = "mixed_bfloat16"
# tf.keras.mixed_precision.set_global_policy(mixed_precision_policy)

# import os
# from tensorflow.python.profiler import profiler_client

# tpu_profile_service_address = os.environ['COLAB_TPU_ADDR'].replace('8470', '8466')
# print(profiler_client.monitor(tpu_profile_service_address, 100, 2))


import re
import os
import numpy as np
from matplotlib import pyplot as plt
from tensorflow import keras
from tensorflow.keras import layers
from official.common import distribute_utils
import tensorflow_addons as tfa
from absl import app
import time
import sys
from custom_vit_model import build_model

num_attention_heads = 16
hidden_size = 1280
num_layers = 32

import argparse
parser = argparse.ArgumentParser()
parser.add_argument("--bits", type=int)
parser.add_argument("--micro_batch_size", type=int)
args = parser.parse_args()
micro_batch_size = args.micro_batch_size  # batch size per TPU core
bits = args.bits
assert bits in [16, 32]
if bits == 16:
    global_dtype = tf.bfloat16
    dtype_str = "bfloat16"
elif bits == 32:
    global_dtype = tf.float32
    dtype_str = "float32"
sys.argv = sys.argv[:-2]

num_epochs = 10
learning_rate = 0.001

image_size = 224
patch_size = 16  # Size of the patches to be extract from the input images


"""## Training
Actually train the model. The first epoch will be quite a bit slower as we must XLA-compile the execution graph and load the data.
"""
def run():
    print("Working on: bits: {}, micro_batch_size: {}".format(bits, micro_batch_size))

    # # Start TF profiler server.
    # profiler_port = 9012
    # tf.profiler.experimental.server.start(profiler_port)

    strategy = distribute_utils.get_distribution_strategy(
        distribution_strategy="tpu",
        num_gpus=1,  # How many GPUs to use at each worker with the DistributionStrategies API. The default is 1.
        tpu_address=os.environ["TPU_NAME"])

    strategy_scope = distribute_utils.get_strategy_scope(strategy)

    global_batch_size = micro_batch_size * strategy.num_replicas_in_sync

    # Input data
    num_examples = global_batch_size * 2
    num_steps = num_examples / global_batch_size
    num_classes = 1000  # Default in Megatron ViT
    input_shape = (image_size, image_size, 3)

    train_examples = np.zeros(shape=(num_examples, *input_shape), dtype=np.float32).astype(global_dtype.as_numpy_dtype)
    train_labels = np.random.randint(0, num_classes, size=(num_examples, 1))
    # See https://keras.io/guides/distributed_training/ for how to set batch size
    train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels)).batch(global_batch_size).repeat(10).prefetch(2)

    with strategy_scope: # creating the model in the TPUStrategy scope means we will train the model on the TPU
        model = build_model(
            image_size_tuple=(image_size, image_size),
            patch_size=patch_size,
            num_layers=num_layers,
            hidden_size=hidden_size,
            num_heads=num_attention_heads,
            mlp_dim=4*hidden_size,
            classes=num_classes,
            dropout=0.,
            dtype=global_dtype,
            dtype_str=dtype_str,
        )
        # print(model.summary())
        optimizer = tf.optimizers.Adam(learning_rate=learning_rate)
        metrics = []
        model.compile(
            optimizer=optimizer,
            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),  # Do we use the same loss in Megatron for ViT?
            metrics=metrics,
        )


    # Warm up
    history = model.fit(
        train_examples,
        train_labels,
        # train_dataset,
        batch_size=global_batch_size,
        epochs=5,
        callbacks=[],
    )

    delta = 5
    start_time = time.time()
    history = model.fit(
        train_examples,
        train_labels,
        # train_dataset,
        batch_size=global_batch_size,
        epochs=3,
        callbacks=[],
    )
    first_epoch_group_time = time.time() - start_time

    start_time = time.time()
    history = model.fit(
        train_examples,
        train_labels,
        # train_dataset,
        batch_size=global_batch_size,
        epochs=3 + delta,
        callbacks=[],
    )
    second_epoch_group_time = time.time() - start_time
    print("bits: {}, micro_batch_size: {}, time per step (s): {:.3f}".format(bits, micro_batch_size, (second_epoch_group_time - first_epoch_group_time) / delta / num_steps))

    return history


def main(_):
  run()


if __name__ == '__main__':
  app.run(main)
